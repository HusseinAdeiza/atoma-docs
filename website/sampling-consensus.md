# Atoma's Sampling Consensus

In order to process requests, the Atoma Network protocol selects a given
number of nodes uniformly at random. The selected nodes must execute the
given requests, otherwise part of their collateral value is at risk
of being slashed. Moreover, the selected nodes are guaranteed to have similar hardware and software specifications. The node sampling mechanism is part of our `Sampling Consensus` protocol. The latter allows the Atoma Network to satisfy the following features:

1. Load balancing of the request volume across the Atoma Network. As requests are only executed across nodes with specific hardware capacities, this node selection homogeneizes the balancing of the request load on the Atoma Network.

2. The selection mechanism allows nodes to reach deterministic agreement on the state of the output generated by executing the given request. This means that all selected nodes will generate the same outputs, given the same inputs and the same workload (specified by the incoming request). This is specially important for workloads that rely on floating point arithmetic, the latter being highly non-deterministic. Such an important example regards AI inference, which depends crucially on floating point arithmetic (specially for LLM inference). However, if we select nodes with the same hardware specifications (such as, the same GPU card), determinism of floating point arithmetic is guaranteed (with potential mild changes to the kernels which the code runs into).

This is especially relevant to enforce verifiable compute. Indeed, by enforcing determinism across processed requests, we can rely on the following observation:

`Observation:` Through the Atoma sampling consensus protocol, whenever two or more nodes are selected to execute a request, the select nodes disagree if at least one of the node has been dishonest. In particular, a honest node will never agree on the state of a request output with dishonest one.

## Probabilistic considerations for Atoma's Sampling Consensus

As previously mentioned, the Atoma protocol can enforce determinism for
execution workloads (a.k.a requests) by compute replication, through selecting uniformly at random a given number of nodes to run the same compute. 

Depending on the number of sampled nodes, different levels of output integrity are reached. For example, assuming a dishonest participant of the
Atoma Network controls a percentage `r` of the network, the probability, `P`, that a quorum of `N > 0` selected nodes (including at least one of the dishonest participant nodes) is

`P = r^N`.

In particular, if `N = 5` nodes are selected and the dishonest participant controls one third of the network, that is `r = 1/3`, the probability above becomes

`P = (1/3)^5 = 0.00411..`

with `N = 10`, the probability becomes

`P = (1/3)^10 = 1.6935e-05`,

and so on and so forth. We can see that even a small set of nodes, selected uniformly at random, can lead to very high trust guarantees that a given generated output has not been tampered with, in any possible form.

## Cross validation mechanism

## Node obfuscation


